{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e79247",
   "metadata": {
    "id": "a3e79247"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as td\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import image\n",
    "from matplotlib import pyplot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "FNa7s6OrU8A1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNa7s6OrU8A1",
    "outputId": "d4624c15-2123-46ca-e03a-88b67bfb1ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308235d6",
   "metadata": {
    "id": "308235d6"
   },
   "outputs": [],
   "source": [
    "from numpy.core.fromnumeric import shape\n",
    "def load_data(path,batch_size,input_size):\n",
    "    \n",
    "    normalize = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]) \n",
    "    transform_dict = {\"src\":  normalize}  \n",
    "    # train_path=path+\"/train\"\n",
    "    # test_path=path+\"/test\"\n",
    "    data = datasets.ImageFolder(root=path,transform=transform_dict[\"src\"])\n",
    "    # transform_dict1 = {\"test\":  normalize} \n",
    "    # test1 = datasets.ImageFolder(root=path,transform=transform_dict[\"test\"])\n",
    "    # transform_dict = {\"test\":  normalize}\n",
    "    # test= datasets.ImageFolder(root=path,transform=transform_dict[\"test\"])\n",
    "    # train_size = int((1- (test_split + val_split)) * len(data))\n",
    "    # test_size = int((1 - (val_split)) * len(data)) - train_size\n",
    "    # val_size = len(data) - train_size - test_size\n",
    "    train_size=int(0.75*len(data))\n",
    "    print(len(data))\n",
    "    test_size=int(len(data)-train_size)\n",
    "    train, test = td.random_split(data,[train_size,test_size])\n",
    "\n",
    "    data_loader_train = td.DataLoader(train,batch_size=batch_size,shuffle=True,drop_last=False,num_workers=0)\n",
    "    data_loader_test = td.DataLoader(test,batch_size=batch_size,shuffle=True,drop_last=False,num_workers=0)\n",
    "    # data_loader_val = td.DataLoader(val,batch_size=batch_size,shuffle=True,drop_last=False,num_workers=0)\n",
    "    return data_loader_train, data_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b259d067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21165\n"
     ]
    }
   ],
   "source": [
    "data_loader_train,data_loader_test=load_data(r\"C:\\Users\\Admin\\Desktop\\COMP 6721\\Project\\datasets\\Dataset1\",32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cae6e61b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cae6e61b",
    "outputId": "7f5eb62c-7288-4c73-ad4d-913eb2103dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6922\n"
     ]
    }
   ],
   "source": [
    "data_loader_train,data_loader_test=load_data(r\"/content/drive/MyDrive/Project/datasets/Dataset2\",32,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16170f93",
   "metadata": {
    "id": "16170f93"
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 4):\n",
    "        super().__init__()\n",
    "#         _log_api_usage_once(self)\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            torch.nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            torch.nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256 * 6 * 6, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2b7e6f6",
   "metadata": {
    "id": "f2b7e6f6"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "net = AlexNet(3).to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efa56f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8efa56f0",
    "outputId": "d5f31bda-3c17-4fbe-bc9a-462d69c5fc79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch [1/20], Step [10/163], Loss: 1.0981, Accuracy: 40.62%\n",
      "Epoch [1/20], Step [20/163], Loss: 1.0981, Accuracy: 25.00%\n",
      "Epoch [1/20], Step [30/163], Loss: 1.0626, Accuracy: 50.00%\n",
      "Epoch [1/20], Step [40/163], Loss: 1.0912, Accuracy: 34.38%\n",
      "Epoch [1/20], Step [50/163], Loss: 1.0934, Accuracy: 37.50%\n",
      "Epoch [1/20], Step [60/163], Loss: 1.1392, Accuracy: 18.75%\n",
      "Epoch [1/20], Step [70/163], Loss: 1.0302, Accuracy: 31.25%\n",
      "Epoch [1/20], Step [80/163], Loss: 1.1423, Accuracy: 31.25%\n",
      "Epoch [1/20], Step [90/163], Loss: 1.0073, Accuracy: 50.00%\n",
      "Epoch [1/20], Step [100/163], Loss: 1.1445, Accuracy: 28.12%\n",
      "Epoch [1/20], Step [110/163], Loss: 1.0799, Accuracy: 40.62%\n",
      "Epoch [1/20], Step [120/163], Loss: 1.1202, Accuracy: 43.75%\n",
      "Epoch [1/20], Step [130/163], Loss: 1.0691, Accuracy: 40.62%\n",
      "Epoch [1/20], Step [140/163], Loss: 1.0952, Accuracy: 46.88%\n",
      "Epoch [1/20], Step [150/163], Loss: 1.0431, Accuracy: 62.50%\n",
      "Epoch [1/20], Step [160/163], Loss: 1.1520, Accuracy: 40.62%\n",
      "Epoch [2/20], Step [10/163], Loss: 0.8445, Accuracy: 62.50%\n",
      "Epoch [2/20], Step [20/163], Loss: 1.0541, Accuracy: 46.88%\n",
      "Epoch [2/20], Step [30/163], Loss: 1.4761, Accuracy: 18.75%\n",
      "Epoch [2/20], Step [40/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [2/20], Step [50/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [2/20], Step [60/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [2/20], Step [70/163], Loss: nan, Accuracy: 65.62%\n",
      "Epoch [2/20], Step [80/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [2/20], Step [90/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [2/20], Step [100/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [2/20], Step [110/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [2/20], Step [120/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [2/20], Step [130/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [2/20], Step [140/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [2/20], Step [150/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [2/20], Step [160/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [3/20], Step [10/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [3/20], Step [20/163], Loss: nan, Accuracy: 3.12%\n",
      "Epoch [3/20], Step [30/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [3/20], Step [40/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [3/20], Step [50/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [3/20], Step [60/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [3/20], Step [70/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [3/20], Step [80/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [3/20], Step [90/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [3/20], Step [100/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [3/20], Step [110/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [3/20], Step [120/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [3/20], Step [130/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [3/20], Step [140/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [3/20], Step [150/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [3/20], Step [160/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [4/20], Step [10/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [4/20], Step [20/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [4/20], Step [30/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [4/20], Step [40/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [4/20], Step [50/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [4/20], Step [60/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [4/20], Step [70/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [4/20], Step [80/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [4/20], Step [90/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [4/20], Step [100/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [4/20], Step [110/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [4/20], Step [120/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [4/20], Step [130/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [4/20], Step [140/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [4/20], Step [150/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [4/20], Step [160/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [5/20], Step [10/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [5/20], Step [20/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [5/20], Step [30/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [5/20], Step [40/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [5/20], Step [50/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [5/20], Step [60/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [5/20], Step [70/163], Loss: nan, Accuracy: 46.88%\n",
      "Epoch [5/20], Step [80/163], Loss: nan, Accuracy: 46.88%\n",
      "Epoch [5/20], Step [90/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [5/20], Step [100/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [5/20], Step [110/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [5/20], Step [120/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [5/20], Step [130/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [5/20], Step [140/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [5/20], Step [150/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [5/20], Step [160/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [6/20], Step [10/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [6/20], Step [20/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [6/20], Step [30/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [6/20], Step [40/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [6/20], Step [50/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [6/20], Step [60/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [6/20], Step [70/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [6/20], Step [80/163], Loss: nan, Accuracy: 50.00%\n",
      "Epoch [6/20], Step [90/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [6/20], Step [100/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [6/20], Step [110/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [6/20], Step [120/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [6/20], Step [130/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [6/20], Step [140/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [6/20], Step [150/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [6/20], Step [160/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [7/20], Step [10/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [7/20], Step [20/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [7/20], Step [30/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [7/20], Step [40/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [7/20], Step [50/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [7/20], Step [60/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [7/20], Step [70/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [7/20], Step [80/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [7/20], Step [90/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [7/20], Step [100/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [7/20], Step [110/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [7/20], Step [120/163], Loss: nan, Accuracy: 56.25%\n",
      "Epoch [7/20], Step [130/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [7/20], Step [140/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [7/20], Step [150/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [7/20], Step [160/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [8/20], Step [10/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [8/20], Step [20/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [8/20], Step [30/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [8/20], Step [40/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [8/20], Step [50/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [8/20], Step [60/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [8/20], Step [70/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [8/20], Step [80/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [8/20], Step [90/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [8/20], Step [100/163], Loss: nan, Accuracy: 53.12%\n",
      "Epoch [8/20], Step [110/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [8/20], Step [120/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [8/20], Step [130/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [8/20], Step [140/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [8/20], Step [150/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [8/20], Step [160/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [9/20], Step [10/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [9/20], Step [20/163], Loss: nan, Accuracy: 50.00%\n",
      "Epoch [9/20], Step [30/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [9/20], Step [40/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [9/20], Step [50/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [9/20], Step [60/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [9/20], Step [70/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [9/20], Step [80/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [9/20], Step [90/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [9/20], Step [100/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [9/20], Step [110/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [9/20], Step [120/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [9/20], Step [130/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [9/20], Step [140/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [9/20], Step [150/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [9/20], Step [160/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [10/20], Step [10/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [10/20], Step [20/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [10/20], Step [30/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [10/20], Step [40/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [10/20], Step [50/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [10/20], Step [60/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [10/20], Step [70/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [10/20], Step [80/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [10/20], Step [90/163], Loss: nan, Accuracy: 56.25%\n",
      "Epoch [10/20], Step [100/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [10/20], Step [110/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [10/20], Step [120/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [10/20], Step [130/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [10/20], Step [140/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [10/20], Step [150/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [10/20], Step [160/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [11/20], Step [10/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [11/20], Step [20/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [11/20], Step [30/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [11/20], Step [40/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [11/20], Step [50/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [11/20], Step [60/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [11/20], Step [70/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [11/20], Step [80/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [11/20], Step [90/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [11/20], Step [100/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [11/20], Step [110/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [11/20], Step [120/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [11/20], Step [130/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [11/20], Step [140/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [11/20], Step [150/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [11/20], Step [160/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [12/20], Step [10/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [12/20], Step [20/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [12/20], Step [30/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [12/20], Step [40/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [12/20], Step [50/163], Loss: nan, Accuracy: 56.25%\n",
      "Epoch [12/20], Step [60/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [12/20], Step [70/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [12/20], Step [80/163], Loss: nan, Accuracy: 46.88%\n",
      "Epoch [12/20], Step [90/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [12/20], Step [100/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [12/20], Step [110/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [12/20], Step [120/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [12/20], Step [130/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [12/20], Step [140/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [12/20], Step [150/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [12/20], Step [160/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [13/20], Step [10/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [13/20], Step [20/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [13/20], Step [30/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [13/20], Step [40/163], Loss: nan, Accuracy: 53.12%\n",
      "Epoch [13/20], Step [50/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [13/20], Step [60/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [13/20], Step [70/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [13/20], Step [80/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [13/20], Step [90/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [13/20], Step [100/163], Loss: nan, Accuracy: 15.62%\n",
      "Epoch [13/20], Step [110/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [13/20], Step [120/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [13/20], Step [130/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [13/20], Step [140/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [13/20], Step [150/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [13/20], Step [160/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [14/20], Step [10/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [14/20], Step [20/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [14/20], Step [30/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [14/20], Step [40/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [14/20], Step [50/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [14/20], Step [60/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [14/20], Step [70/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [14/20], Step [80/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [14/20], Step [90/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [14/20], Step [100/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [14/20], Step [110/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [14/20], Step [120/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [14/20], Step [130/163], Loss: nan, Accuracy: 46.88%\n",
      "Epoch [14/20], Step [140/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [14/20], Step [150/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [14/20], Step [160/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [15/20], Step [10/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [15/20], Step [20/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [15/20], Step [30/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [15/20], Step [40/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [15/20], Step [50/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [15/20], Step [60/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [15/20], Step [70/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [15/20], Step [80/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [15/20], Step [90/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [15/20], Step [100/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [15/20], Step [110/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [15/20], Step [120/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [15/20], Step [130/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [15/20], Step [140/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [15/20], Step [150/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [15/20], Step [160/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [16/20], Step [10/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [16/20], Step [20/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [16/20], Step [30/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [16/20], Step [40/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [16/20], Step [50/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [16/20], Step [60/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [16/20], Step [70/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [16/20], Step [80/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [16/20], Step [90/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [16/20], Step [100/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [16/20], Step [110/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [16/20], Step [120/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [16/20], Step [130/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [16/20], Step [140/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [16/20], Step [150/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [16/20], Step [160/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [17/20], Step [10/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [17/20], Step [20/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [17/20], Step [30/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [17/20], Step [40/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [17/20], Step [50/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [17/20], Step [60/163], Loss: nan, Accuracy: 15.62%\n",
      "Epoch [17/20], Step [70/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [17/20], Step [80/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [17/20], Step [90/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [17/20], Step [100/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [17/20], Step [110/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [17/20], Step [120/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [17/20], Step [130/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [17/20], Step [140/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [17/20], Step [150/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [17/20], Step [160/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [18/20], Step [10/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [18/20], Step [20/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [18/20], Step [30/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [18/20], Step [40/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [18/20], Step [50/163], Loss: nan, Accuracy: 46.88%\n",
      "Epoch [18/20], Step [60/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [18/20], Step [70/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [18/20], Step [80/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [18/20], Step [90/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [18/20], Step [100/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [18/20], Step [110/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [18/20], Step [120/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [18/20], Step [130/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [18/20], Step [140/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [18/20], Step [150/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [18/20], Step [160/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [19/20], Step [10/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [19/20], Step [20/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [19/20], Step [30/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [19/20], Step [40/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [19/20], Step [50/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [19/20], Step [60/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [19/20], Step [70/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [19/20], Step [80/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [19/20], Step [90/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [19/20], Step [100/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [19/20], Step [110/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [19/20], Step [120/163], Loss: nan, Accuracy: 56.25%\n",
      "Epoch [19/20], Step [130/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [19/20], Step [140/163], Loss: nan, Accuracy: 15.62%\n",
      "Epoch [19/20], Step [150/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [19/20], Step [160/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [20/20], Step [10/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [20/20], Step [20/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [20/20], Step [30/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [20/20], Step [40/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [20/20], Step [50/163], Loss: nan, Accuracy: 46.88%\n",
      "Epoch [20/20], Step [60/163], Loss: nan, Accuracy: 21.88%\n",
      "Epoch [20/20], Step [70/163], Loss: nan, Accuracy: 40.62%\n",
      "Epoch [20/20], Step [80/163], Loss: nan, Accuracy: 37.50%\n",
      "Epoch [20/20], Step [90/163], Loss: nan, Accuracy: 18.75%\n",
      "Epoch [20/20], Step [100/163], Loss: nan, Accuracy: 31.25%\n",
      "Epoch [20/20], Step [110/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [20/20], Step [120/163], Loss: nan, Accuracy: 25.00%\n",
      "Epoch [20/20], Step [130/163], Loss: nan, Accuracy: 34.38%\n",
      "Epoch [20/20], Step [140/163], Loss: nan, Accuracy: 28.12%\n",
      "Epoch [20/20], Step [150/163], Loss: nan, Accuracy: 43.75%\n",
      "Epoch [20/20], Step [160/163], Loss: nan, Accuracy: 34.38%\n",
      "######## Training Finished in 2696.7633385658264 seconds ###########\n"
     ]
    }
   ],
   "source": [
    "###### Define and run your training loop here #########\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "net.to(device)\n",
    "\n",
    "num_epochs = 20\n",
    "total_steps = len(data_loader_train)\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(data_loader_train):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # Forward pass\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(\"kjnfjnrnkrn\",i)\\\\\n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(\"second time\",i)\n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),\n",
    "                    (correct / total) * 100))\n",
    "            \n",
    "print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed2f139e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed2f139e",
    "outputId": "530912f3-e0a7-41fd-b363-bab850c76395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1731 test images: 33.10225303292894 %\n"
     ]
    }
   ],
   "source": [
    "######## Write your code here #############\n",
    "net.eval() \n",
    "count=0\n",
    "with torch.no_grad(): \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in data_loader_test:\n",
    "        # count+=1\n",
    "        # print(count)\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the {} test images: {} %'\n",
    "        .format(total, (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b09bbf",
   "metadata": {},
   "source": [
    "# Training the dataset with AlexNet fetching directly from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc80cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Admin/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9cb3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Epoch [1/5], Step [10/497], Loss: 6.7373, Accuracy: 50.00%\n",
      "Epoch [1/5], Step [20/497], Loss: 4.8142, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [30/497], Loss: 5.1484, Accuracy: 43.75%\n",
      "Epoch [1/5], Step [40/497], Loss: 1.7266, Accuracy: 18.75%\n",
      "Epoch [1/5], Step [50/497], Loss: 1.3228, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [60/497], Loss: 1.2216, Accuracy: 50.00%\n",
      "Epoch [1/5], Step [70/497], Loss: 0.9579, Accuracy: 68.75%\n",
      "Epoch [1/5], Step [80/497], Loss: 1.1964, Accuracy: 59.38%\n",
      "Epoch [1/5], Step [90/497], Loss: 1.3997, Accuracy: 40.62%\n",
      "Epoch [1/5], Step [100/497], Loss: 1.3866, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [110/497], Loss: 1.0768, Accuracy: 53.12%\n",
      "Epoch [1/5], Step [120/497], Loss: 1.2694, Accuracy: 21.88%\n",
      "Epoch [1/5], Step [130/497], Loss: 1.1974, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [140/497], Loss: 1.4556, Accuracy: 37.50%\n",
      "Epoch [1/5], Step [150/497], Loss: 1.2073, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [160/497], Loss: 1.2672, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [170/497], Loss: 0.9469, Accuracy: 62.50%\n",
      "Epoch [1/5], Step [180/497], Loss: 1.2260, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [190/497], Loss: 1.1884, Accuracy: 46.88%\n",
      "Epoch [1/5], Step [200/497], Loss: 1.1933, Accuracy: 40.62%\n",
      "Epoch [1/5], Step [210/497], Loss: 1.4403, Accuracy: 37.50%\n",
      "Epoch [1/5], Step [220/497], Loss: 1.0341, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [230/497], Loss: 0.8658, Accuracy: 71.88%\n",
      "Epoch [1/5], Step [240/497], Loss: 1.3569, Accuracy: 34.38%\n",
      "Epoch [1/5], Step [250/497], Loss: 1.1929, Accuracy: 59.38%\n",
      "Epoch [1/5], Step [260/497], Loss: 0.8195, Accuracy: 68.75%\n",
      "Epoch [1/5], Step [270/497], Loss: 1.1804, Accuracy: 40.62%\n",
      "Epoch [1/5], Step [280/497], Loss: 1.0842, Accuracy: 62.50%\n",
      "Epoch [1/5], Step [290/497], Loss: 1.1782, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [300/497], Loss: 0.8369, Accuracy: 75.00%\n",
      "Epoch [1/5], Step [310/497], Loss: 0.8992, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [320/497], Loss: 0.9406, Accuracy: 62.50%\n",
      "Epoch [1/5], Step [330/497], Loss: 0.9602, Accuracy: 53.12%\n",
      "Epoch [1/5], Step [340/497], Loss: 0.9865, Accuracy: 68.75%\n",
      "Epoch [1/5], Step [350/497], Loss: 0.9003, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [360/497], Loss: 0.6637, Accuracy: 68.75%\n",
      "Epoch [1/5], Step [370/497], Loss: 0.7966, Accuracy: 71.88%\n",
      "Epoch [1/5], Step [380/497], Loss: 0.9510, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [390/497], Loss: 0.8916, Accuracy: 68.75%\n",
      "Epoch [1/5], Step [400/497], Loss: 0.6225, Accuracy: 81.25%\n",
      "Epoch [1/5], Step [410/497], Loss: 0.9746, Accuracy: 53.12%\n",
      "Epoch [1/5], Step [420/497], Loss: 0.5727, Accuracy: 78.12%\n",
      "Epoch [1/5], Step [430/497], Loss: 0.8721, Accuracy: 62.50%\n",
      "Epoch [1/5], Step [440/497], Loss: 0.5109, Accuracy: 78.12%\n",
      "Epoch [1/5], Step [450/497], Loss: 0.7549, Accuracy: 71.88%\n",
      "Epoch [1/5], Step [460/497], Loss: 0.9426, Accuracy: 56.25%\n",
      "Epoch [1/5], Step [470/497], Loss: 0.6581, Accuracy: 78.12%\n",
      "Epoch [1/5], Step [480/497], Loss: 0.9912, Accuracy: 53.12%\n",
      "Epoch [1/5], Step [490/497], Loss: 0.5806, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [10/497], Loss: 0.8144, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [20/497], Loss: 0.9988, Accuracy: 59.38%\n",
      "Epoch [2/5], Step [30/497], Loss: 0.7306, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [40/497], Loss: 0.8896, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [50/497], Loss: 0.8780, Accuracy: 65.62%\n",
      "Epoch [2/5], Step [60/497], Loss: 0.7492, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [70/497], Loss: 0.9314, Accuracy: 62.50%\n",
      "Epoch [2/5], Step [80/497], Loss: 0.8714, Accuracy: 62.50%\n",
      "Epoch [2/5], Step [90/497], Loss: 0.9080, Accuracy: 62.50%\n",
      "Epoch [2/5], Step [100/497], Loss: 0.7201, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [110/497], Loss: 0.6711, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [120/497], Loss: 0.7279, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [130/497], Loss: 0.9893, Accuracy: 62.50%\n",
      "Epoch [2/5], Step [140/497], Loss: 0.5920, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [150/497], Loss: 0.9767, Accuracy: 62.50%\n",
      "Epoch [2/5], Step [160/497], Loss: 0.9077, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [170/497], Loss: 0.7656, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [180/497], Loss: 0.5420, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [190/497], Loss: 0.8525, Accuracy: 62.50%\n",
      "Epoch [2/5], Step [200/497], Loss: 0.7214, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [210/497], Loss: 0.7637, Accuracy: 65.62%\n",
      "Epoch [2/5], Step [220/497], Loss: 0.6213, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [230/497], Loss: 0.5182, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [240/497], Loss: 0.4314, Accuracy: 84.38%\n",
      "Epoch [2/5], Step [250/497], Loss: 0.7642, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [260/497], Loss: 1.0013, Accuracy: 56.25%\n",
      "Epoch [2/5], Step [270/497], Loss: 0.2507, Accuracy: 96.88%\n",
      "Epoch [2/5], Step [280/497], Loss: 0.7511, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [290/497], Loss: 0.5380, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [300/497], Loss: 0.8720, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [310/497], Loss: 0.8475, Accuracy: 65.62%\n",
      "Epoch [2/5], Step [320/497], Loss: 0.7124, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [330/497], Loss: 0.7503, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [340/497], Loss: 0.6908, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [350/497], Loss: 0.7005, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [360/497], Loss: 0.7658, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [370/497], Loss: 0.6196, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [380/497], Loss: 0.7544, Accuracy: 65.62%\n",
      "Epoch [2/5], Step [390/497], Loss: 0.5861, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [400/497], Loss: 0.5182, Accuracy: 81.25%\n",
      "Epoch [2/5], Step [410/497], Loss: 0.6454, Accuracy: 71.88%\n",
      "Epoch [2/5], Step [420/497], Loss: 0.5150, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [430/497], Loss: 0.7526, Accuracy: 68.75%\n",
      "Epoch [2/5], Step [440/497], Loss: 0.5818, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [450/497], Loss: 0.9803, Accuracy: 59.38%\n",
      "Epoch [2/5], Step [460/497], Loss: 0.5856, Accuracy: 75.00%\n",
      "Epoch [2/5], Step [470/497], Loss: 0.5072, Accuracy: 78.12%\n",
      "Epoch [2/5], Step [480/497], Loss: 1.0856, Accuracy: 56.25%\n",
      "Epoch [2/5], Step [490/497], Loss: 0.7997, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [10/497], Loss: 0.7600, Accuracy: 65.62%\n",
      "Epoch [3/5], Step [20/497], Loss: 0.7370, Accuracy: 68.75%\n",
      "Epoch [3/5], Step [30/497], Loss: 0.4040, Accuracy: 90.62%\n",
      "Epoch [3/5], Step [40/497], Loss: 0.6452, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [50/497], Loss: 0.7158, Accuracy: 68.75%\n",
      "Epoch [3/5], Step [60/497], Loss: 0.6299, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [70/497], Loss: 0.4483, Accuracy: 84.38%\n",
      "Epoch [3/5], Step [80/497], Loss: 0.6522, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [90/497], Loss: 0.9313, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [100/497], Loss: 0.4854, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [110/497], Loss: 0.6361, Accuracy: 65.62%\n",
      "Epoch [3/5], Step [120/497], Loss: 0.4712, Accuracy: 84.38%\n",
      "Epoch [3/5], Step [130/497], Loss: 0.5847, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [140/497], Loss: 0.5454, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [150/497], Loss: 0.6939, Accuracy: 68.75%\n",
      "Epoch [3/5], Step [160/497], Loss: 0.7892, Accuracy: 71.88%\n",
      "Epoch [3/5], Step [170/497], Loss: 0.5022, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [180/497], Loss: 0.7632, Accuracy: 68.75%\n",
      "Epoch [3/5], Step [190/497], Loss: 0.5913, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [200/497], Loss: 0.5624, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [210/497], Loss: 0.5557, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [220/497], Loss: 0.4795, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [230/497], Loss: 0.7025, Accuracy: 68.75%\n",
      "Epoch [3/5], Step [240/497], Loss: 0.8973, Accuracy: 71.88%\n",
      "Epoch [3/5], Step [250/497], Loss: 0.6001, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [260/497], Loss: 0.4183, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [270/497], Loss: 0.8381, Accuracy: 71.88%\n",
      "Epoch [3/5], Step [280/497], Loss: 0.4893, Accuracy: 84.38%\n",
      "Epoch [3/5], Step [290/497], Loss: 0.5668, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [300/497], Loss: 0.4940, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [310/497], Loss: 0.6292, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [320/497], Loss: 0.7001, Accuracy: 71.88%\n",
      "Epoch [3/5], Step [330/497], Loss: 0.5145, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [340/497], Loss: 0.7145, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [350/497], Loss: 0.6103, Accuracy: 71.88%\n",
      "Epoch [3/5], Step [360/497], Loss: 0.8727, Accuracy: 62.50%\n",
      "Epoch [3/5], Step [370/497], Loss: 0.6506, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [380/497], Loss: 0.5778, Accuracy: 68.75%\n",
      "Epoch [3/5], Step [390/497], Loss: 0.5795, Accuracy: 81.25%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [400/497], Loss: 0.5106, Accuracy: 90.62%\n",
      "Epoch [3/5], Step [410/497], Loss: 0.6062, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [420/497], Loss: 0.3001, Accuracy: 87.50%\n",
      "Epoch [3/5], Step [430/497], Loss: 0.5465, Accuracy: 84.38%\n",
      "Epoch [3/5], Step [440/497], Loss: 0.6106, Accuracy: 71.88%\n",
      "Epoch [3/5], Step [450/497], Loss: 0.4476, Accuracy: 81.25%\n",
      "Epoch [3/5], Step [460/497], Loss: 0.4473, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [470/497], Loss: 0.5653, Accuracy: 78.12%\n",
      "Epoch [3/5], Step [480/497], Loss: 0.5973, Accuracy: 75.00%\n",
      "Epoch [3/5], Step [490/497], Loss: 0.4427, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [10/497], Loss: 0.9111, Accuracy: 68.75%\n",
      "Epoch [4/5], Step [20/497], Loss: 0.7388, Accuracy: 71.88%\n",
      "Epoch [4/5], Step [30/497], Loss: 0.5872, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [40/497], Loss: 0.9233, Accuracy: 56.25%\n",
      "Epoch [4/5], Step [50/497], Loss: 0.6773, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [60/497], Loss: 1.0131, Accuracy: 46.88%\n",
      "Epoch [4/5], Step [70/497], Loss: 0.7752, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [80/497], Loss: 0.5861, Accuracy: 71.88%\n",
      "Epoch [4/5], Step [90/497], Loss: 0.7751, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [100/497], Loss: 0.5392, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [110/497], Loss: 0.4821, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [120/497], Loss: 0.6794, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [130/497], Loss: 0.7861, Accuracy: 65.62%\n",
      "Epoch [4/5], Step [140/497], Loss: 0.3817, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [150/497], Loss: 0.7352, Accuracy: 65.62%\n",
      "Epoch [4/5], Step [160/497], Loss: 0.4506, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [170/497], Loss: 0.6277, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [180/497], Loss: 0.4884, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [190/497], Loss: 0.6864, Accuracy: 65.62%\n",
      "Epoch [4/5], Step [200/497], Loss: 0.2728, Accuracy: 93.75%\n",
      "Epoch [4/5], Step [210/497], Loss: 0.7241, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [220/497], Loss: 0.5585, Accuracy: 84.38%\n",
      "Epoch [4/5], Step [230/497], Loss: 0.4223, Accuracy: 84.38%\n",
      "Epoch [4/5], Step [240/497], Loss: 0.5999, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [250/497], Loss: 0.6244, Accuracy: 65.62%\n",
      "Epoch [4/5], Step [260/497], Loss: 0.5018, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [270/497], Loss: 0.6613, Accuracy: 71.88%\n",
      "Epoch [4/5], Step [280/497], Loss: 0.2352, Accuracy: 90.62%\n",
      "Epoch [4/5], Step [290/497], Loss: 0.8388, Accuracy: 62.50%\n",
      "Epoch [4/5], Step [300/497], Loss: 0.4623, Accuracy: 78.12%\n",
      "Epoch [4/5], Step [310/497], Loss: 0.5791, Accuracy: 84.38%\n",
      "Epoch [4/5], Step [320/497], Loss: 0.4426, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [330/497], Loss: 0.6454, Accuracy: 71.88%\n",
      "Epoch [4/5], Step [340/497], Loss: 0.8010, Accuracy: 62.50%\n",
      "Epoch [4/5], Step [350/497], Loss: 0.4433, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [360/497], Loss: 0.3802, Accuracy: 84.38%\n",
      "Epoch [4/5], Step [370/497], Loss: 0.4048, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [380/497], Loss: 0.5067, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [390/497], Loss: 0.6279, Accuracy: 68.75%\n",
      "Epoch [4/5], Step [400/497], Loss: 0.7697, Accuracy: 68.75%\n",
      "Epoch [4/5], Step [410/497], Loss: 0.5750, Accuracy: 71.88%\n",
      "Epoch [4/5], Step [420/497], Loss: 0.4053, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [430/497], Loss: 0.6602, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [440/497], Loss: 0.6476, Accuracy: 84.38%\n",
      "Epoch [4/5], Step [450/497], Loss: 0.3878, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [460/497], Loss: 0.5337, Accuracy: 81.25%\n",
      "Epoch [4/5], Step [470/497], Loss: 0.3891, Accuracy: 87.50%\n",
      "Epoch [4/5], Step [480/497], Loss: 0.6545, Accuracy: 75.00%\n",
      "Epoch [4/5], Step [490/497], Loss: 0.9558, Accuracy: 68.75%\n",
      "Epoch [5/5], Step [10/497], Loss: 0.7498, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [20/497], Loss: 0.7555, Accuracy: 68.75%\n",
      "Epoch [5/5], Step [30/497], Loss: 0.5946, Accuracy: 68.75%\n",
      "Epoch [5/5], Step [40/497], Loss: 0.5570, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [50/497], Loss: 0.4420, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [60/497], Loss: 0.5776, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [70/497], Loss: 0.5552, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [80/497], Loss: 0.6879, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [90/497], Loss: 0.5830, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [100/497], Loss: 0.6052, Accuracy: 71.88%\n",
      "Epoch [5/5], Step [110/497], Loss: 0.3411, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [120/497], Loss: 0.5134, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [130/497], Loss: 0.5527, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [140/497], Loss: 0.7705, Accuracy: 68.75%\n",
      "Epoch [5/5], Step [150/497], Loss: 0.4471, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [160/497], Loss: 0.5896, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [170/497], Loss: 0.4861, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [180/497], Loss: 0.5996, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [190/497], Loss: 0.4018, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [200/497], Loss: 0.5613, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [210/497], Loss: 0.4987, Accuracy: 71.88%\n",
      "Epoch [5/5], Step [220/497], Loss: 0.4851, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [230/497], Loss: 0.4500, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [240/497], Loss: 0.5743, Accuracy: 71.88%\n",
      "Epoch [5/5], Step [250/497], Loss: 0.4741, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [260/497], Loss: 0.5693, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [270/497], Loss: 0.4962, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [280/497], Loss: 0.4649, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [290/497], Loss: 0.4414, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [300/497], Loss: 0.3960, Accuracy: 90.62%\n",
      "Epoch [5/5], Step [310/497], Loss: 0.4339, Accuracy: 90.62%\n",
      "Epoch [5/5], Step [320/497], Loss: 0.6929, Accuracy: 68.75%\n",
      "Epoch [5/5], Step [330/497], Loss: 0.4646, Accuracy: 81.25%\n",
      "Epoch [5/5], Step [340/497], Loss: 0.4596, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [350/497], Loss: 0.4674, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [360/497], Loss: 0.3915, Accuracy: 90.62%\n",
      "Epoch [5/5], Step [370/497], Loss: 0.3716, Accuracy: 90.62%\n",
      "Epoch [5/5], Step [380/497], Loss: 0.5495, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [390/497], Loss: 0.3181, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [400/497], Loss: 0.4336, Accuracy: 71.88%\n",
      "Epoch [5/5], Step [410/497], Loss: 0.7769, Accuracy: 65.62%\n",
      "Epoch [5/5], Step [420/497], Loss: 0.3872, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [430/497], Loss: 0.3639, Accuracy: 87.50%\n",
      "Epoch [5/5], Step [440/497], Loss: 0.4364, Accuracy: 78.12%\n",
      "Epoch [5/5], Step [450/497], Loss: 0.5716, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [460/497], Loss: 0.3589, Accuracy: 93.75%\n",
      "Epoch [5/5], Step [470/497], Loss: 0.5450, Accuracy: 84.38%\n",
      "Epoch [5/5], Step [480/497], Loss: 0.6291, Accuracy: 75.00%\n",
      "Epoch [5/5], Step [490/497], Loss: 0.6430, Accuracy: 78.12%\n",
      "######## Training Finished in 450.3462314605713 seconds ###########\n"
     ]
    }
   ],
   "source": [
    "###### Define and run your training loop here #########\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "total_steps = len(data_loader_train)\n",
    "t1 = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(data_loader_train):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print(\"kjnfjnrnkrn\",i)\\\\\n",
    "        # Backprop and optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print(\"second time\",i)\n",
    "        # Train accuracy\n",
    "        total = labels.size(0)\n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                .format(epoch + 1, num_epochs, i + 1, total_steps, loss.item(),\n",
    "                    (correct / total) * 100))\n",
    "            \n",
    "print(\"######## Training Finished in {} seconds ###########\".format(time.time()-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c7a360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 5292 test images: 79.59183673469387 %\n"
     ]
    }
   ],
   "source": [
    "model.eval() \n",
    "perds = []\n",
    "target = []\n",
    "with torch.no_grad(): \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in data_loader_test:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        perds.extend(predicted)\n",
    "        target.extend(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Test Accuracy of the model on the {} test images: {} %'\n",
    "        .format(total, (correct / total) * 100))\n",
    "    \n",
    "# plt.plot(Accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb721861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a79c215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7959, device='cuda:0')\n",
      "(tensor(0.8557, device='cuda:0'), tensor(0.7378, device='cuda:0'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 458,  102,  375,    3],\n",
       "        [  42, 1086,  380,    3],\n",
       "        [  27,   74, 2408,    3],\n",
       "        [   9,   12,   50,  260]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional import precision_recall\n",
    "from torchmetrics import F1Score\n",
    "from torchmetrics import ConfusionMatrix\n",
    "perds1 = torch.stack(perds)\n",
    "target1 = torch.stack(target)\n",
    "f1 = F1Score(num_classes=4).to(device)\n",
    "print(f1(perds1, target1))\n",
    "print(precision_recall(perds1, target1, average='macro', num_classes=4))\n",
    "confmat = ConfusionMatrix(num_classes=4).to(device)\n",
    "confmat(perds1, target1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
